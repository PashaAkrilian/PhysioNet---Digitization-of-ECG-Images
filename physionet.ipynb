{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Loading, Metadata Audit, and Manifests","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport pandas as pd\n\n# ---- paths\nDATA_ROOT = Path(\"/kaggle/input/physionet-ecg-image-digitization\")\nTRAIN_DIR = DATA_ROOT / \"train\"\nTEST_DIR  = DATA_ROOT / \"test\"\n\nOUT_ROOT = Path(\"/kaggle/working/ecgdig\")\nART_DIR  = OUT_ROOT / \"artifacts\"\nOUT_ROOT.mkdir(parents=True, exist_ok=True)\nART_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- hard sanity (no branching; fail fast)\nassert (DATA_ROOT / \"train.csv\").exists()\nassert (DATA_ROOT / \"test.csv\").exists()\nassert (DATA_ROOT / \"sample_submission.parquet\").exists()\nassert TRAIN_DIR.exists()\nassert TEST_DIR.exists()\n\n# ---- load metadata\ndf_train_meta = pd.read_csv(DATA_ROOT / \"train.csv\")\ndf_test_meta  = pd.read_csv(DATA_ROOT / \"test.csv\")\ndf_sub_tpl    = pd.read_parquet(DATA_ROOT / \"sample_submission.parquet\")\n\n# ---- schema checks\nassert {\"id\", \"fs\", \"sig_len\"}.issubset(df_train_meta.columns)\nassert {\"id\", \"lead\", \"fs\", \"number_of_rows\"}.issubset(df_test_meta.columns)\nassert {\"id\", \"value\"}.issubset(df_sub_tpl.columns)\n\n# ---- train manifest: (base_id, variant_tag, image_path, gt_path)\nrows = []\nfor p in sorted(TRAIN_DIR.iterdir()):\n    base_id = p.name\n    gt_path = p / f\"{base_id}.csv\"\n    assert gt_path.exists()\n    for img_path in sorted(p.glob(f\"{base_id}-*.png\")):\n        variant_tag = img_path.stem.split(\"-\")[-1]\n        rows.append((base_id, variant_tag, str(img_path), str(gt_path)))\n\ndf_train_manifest = pd.DataFrame(rows, columns=[\"base_id\", \"variant_tag\", \"image_path\", \"gt_path\"])\nassert len(df_train_manifest) > 0\n\n# ---- test manifest: (base_id, image_path)\ndf_test_manifest = df_test_meta[[\"id\"]].rename(columns={\"id\": \"base_id\"}).copy()\ndf_test_manifest[\"image_path\"] = df_test_manifest[\"base_id\"].map(lambda x: str(TEST_DIR / f\"{x}.png\"))\n\n# ---- lightweight audit report\naudit = {\n    \"data_root\": str(DATA_ROOT),\n    \"n_train_meta\": int(len(df_train_meta)),\n    \"n_test_meta\": int(len(df_test_meta)),\n    \"n_train_variants\": int(len(df_train_manifest)),\n    \"train_fs_unique\": sorted(df_train_meta[\"fs\"].unique().tolist()),\n    \"test_fs_unique\": sorted(df_test_meta[\"fs\"].unique().tolist()),\n    \"test_leads_unique\": sorted(df_test_meta[\"lead\"].unique().tolist()),\n    \"sub_template_rows\": int(len(df_sub_tpl)),\n    \"train_manifest_example\": df_train_manifest.head(3).to_dict(orient=\"records\"),\n}\n\n# ---- save artifacts\ndf_train_meta.to_parquet(ART_DIR / \"train_meta.parquet\", index=False)\ndf_test_meta.to_parquet(ART_DIR / \"test_meta.parquet\", index=False)\ndf_sub_tpl.to_parquet(ART_DIR / \"sample_submission.parquet\", index=False)\ndf_train_manifest.to_parquet(ART_DIR / \"train_manifest.parquet\", index=False)\ndf_test_manifest.to_parquet(ART_DIR / \"test_manifest.parquet\", index=False)\n\n(ART_DIR / \"paths.json\").write_text(json.dumps({\n    \"DATA_ROOT\": str(DATA_ROOT),\n    \"TRAIN_DIR\": str(TRAIN_DIR),\n    \"TEST_DIR\": str(TEST_DIR),\n    \"OUT_ROOT\": str(OUT_ROOT),\n    \"ART_DIR\": str(ART_DIR),\n}, indent=2))\n\n(ART_DIR / \"meta_audit.json\").write_text(json.dumps(audit, indent=2))\n\nprint(\"OK | Saved:\")\nprint(\" -\", ART_DIR / \"train_meta.parquet\")\nprint(\" -\", ART_DIR / \"test_meta.parquet\")\nprint(\" -\", ART_DIR / \"train_manifest.parquet\")\nprint(\" -\", ART_DIR / \"test_manifest.parquet\")\nprint(\" -\", ART_DIR / \"sample_submission.parquet\")\nprint(\" -\", ART_DIR / \"meta_audit.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T12:58:09.397762Z","iopub.execute_input":"2026-01-21T12:58:09.398514Z","iopub.status.idle":"2026-01-21T12:58:20.356463Z","shell.execute_reply.started":"2026-01-21T12:58:09.398476Z","shell.execute_reply":"2026-01-21T12:58:20.355542Z"}},"outputs":[{"name":"stdout","text":"OK | Saved:\n - /kaggle/working/ecgdig/artifacts/train_meta.parquet\n - /kaggle/working/ecgdig/artifacts/test_meta.parquet\n - /kaggle/working/ecgdig/artifacts/train_manifest.parquet\n - /kaggle/working/ecgdig/artifacts/test_manifest.parquet\n - /kaggle/working/ecgdig/artifacts/sample_submission.parquet\n - /kaggle/working/ecgdig/artifacts/meta_audit.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Leakage-Safe Cross-Validation and Data Tables","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\n\nOUT_ROOT = Path(\"/kaggle/working/ecgdig\")\nART_DIR  = OUT_ROOT / \"artifacts\"\nassert ART_DIR.exists()\n\n# ---- load stage-1 artifacts (fail fast)\ndf_train_meta     = pd.read_parquet(ART_DIR / \"train_meta.parquet\")\ndf_train_manifest = pd.read_parquet(ART_DIR / \"train_manifest.parquet\")\nassert {\"id\",\"fs\",\"sig_len\"}.issubset(df_train_meta.columns)\nassert {\"base_id\",\"variant_tag\",\"image_path\",\"gt_path\"}.issubset(df_train_manifest.columns)\n\n# ---- reconcile base_id universe (use intersection; no hard assumption)\nbase_from_manifest = pd.Index(df_train_manifest[\"base_id\"].unique())\nbase_from_meta     = pd.Index(df_train_meta[\"id\"].astype(str).unique())\nbase_keep          = pd.Index(sorted(set(base_from_manifest).intersection(set(base_from_meta))))\nassert len(base_keep) > 0\n\n# ---- base table (record-level, only for base_keep)\ndf_base = (df_train_meta[[\"id\",\"fs\",\"sig_len\"]]\n           .assign(id=lambda d: d[\"id\"].astype(str))\n           .rename(columns={\"id\":\"base_id\"})\n           .drop_duplicates(\"base_id\", keep=\"first\"))\n\ndf_base = df_base[df_base[\"base_id\"].isin(base_keep)].reset_index(drop=True)\nassert df_base[\"base_id\"].nunique() == len(df_base)\n\n# ---- manifest restricted to base_keep (ensures fold merge cannot produce NaN)\ndf_train_manifest = df_train_manifest[df_train_manifest[\"base_id\"].isin(base_keep)].reset_index(drop=True)\nassert df_train_manifest[\"base_id\"].nunique() == df_base[\"base_id\"].nunique()\n\n# ---- GroupKFold by base_id (all variants of a base_id share the same fold)\nN_SPLITS = 5\nassert df_base[\"base_id\"].nunique() >= N_SPLITS\n\ngkf  = GroupKFold(n_splits=N_SPLITS)\nfold = np.full(len(df_base), -1, dtype=np.int16)\n\nfor k, (_, va_idx) in enumerate(gkf.split(df_base, groups=df_base[\"base_id\"].values)):\n    fold[va_idx] = k\n\ndf_base[\"fold\"] = fold\nassert (df_base[\"fold\"] >= 0).all()\n\n# ---- attach folds to manifest + build training tables\ndf_train_manifest_folds = df_train_manifest.merge(df_base[[\"base_id\",\"fold\"]], on=\"base_id\", how=\"inner\")\nassert len(df_train_manifest_folds) == len(df_train_manifest)\n\ndf_train_img_table = df_train_manifest_folds.merge(df_base[[\"base_id\",\"fs\",\"sig_len\"]], on=\"base_id\", how=\"inner\")\nassert len(df_train_img_table) == len(df_train_manifest_folds)\n\n# ---- mismatch report (for debugging; does not stop pipeline)\nmissing_in_meta = sorted(set(base_from_manifest) - set(base_from_meta))\nmissing_in_manifest = sorted(set(base_from_meta) - set(base_from_manifest))\nmismatch_report = {\n    \"n_base_ids_manifest\": int(len(base_from_manifest)),\n    \"n_base_ids_meta\": int(len(base_from_meta)),\n    \"n_base_ids_used_intersection\": int(len(base_keep)),\n    \"n_missing_in_train_csv\": int(len(missing_in_meta)),\n    \"n_missing_in_train_folder\": int(len(missing_in_manifest)),\n    \"missing_in_train_csv_head200\": missing_in_meta[:200],\n    \"missing_in_train_folder_head200\": missing_in_manifest[:200],\n}\n\n# ---- fold stats\nfold_stats = {\n    \"n_splits\": int(N_SPLITS),\n    \"n_base_ids_used\": int(df_base[\"base_id\"].nunique()),\n    \"n_train_variants_used\": int(len(df_train_manifest_folds)),\n    \"base_ids_per_fold\": df_base.groupby(\"fold\")[\"base_id\"].nunique().sort_index().astype(int).to_dict(),\n    \"variants_per_fold\": df_train_manifest_folds.groupby(\"fold\")[\"base_id\"].size().sort_index().astype(int).to_dict(),\n    \"fs_unique\": sorted(df_base[\"fs\"].unique().tolist()),\n    \"sig_len_unique\": sorted(df_base[\"sig_len\"].unique().tolist()),\n}\n\n# ---- save artifacts\ndf_base.to_parquet(ART_DIR / \"train_base_folds.parquet\", index=False)\ndf_train_manifest_folds.to_parquet(ART_DIR / \"train_manifest_folds.parquet\", index=False)\ndf_train_img_table.to_parquet(ART_DIR / \"train_img_table.parquet\", index=False)\n(ART_DIR / \"fold_stats.json\").write_text(json.dumps(fold_stats, indent=2))\n(ART_DIR / \"mismatch_report.json\").write_text(json.dumps(mismatch_report, indent=2))\n\nprint(\"OK | Saved:\")\nprint(\" -\", ART_DIR / \"train_base_folds.parquet\")\nprint(\" -\", ART_DIR / \"train_manifest_folds.parquet\")\nprint(\" -\", ART_DIR / \"train_img_table.parquet\")\nprint(\" -\", ART_DIR / \"fold_stats.json\")\nprint(\" -\", ART_DIR / \"mismatch_report.json\")\nprint(\"Used base_ids:\", fold_stats[\"n_base_ids_used\"], \"| Used variants:\", fold_stats[\"n_train_variants_used\"])\nprint(\"Fold base_ids_per_fold:\", fold_stats[\"base_ids_per_fold\"])\nprint(\"Mismatch (manifest not in train.csv):\", mismatch_report[\"n_missing_in_train_csv\"])\nprint(\"Mismatch (train.csv not in manifest):\", mismatch_report[\"n_missing_in_train_folder\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T12:58:20.358870Z","iopub.execute_input":"2026-01-21T12:58:20.359221Z","iopub.status.idle":"2026-01-21T12:58:21.295882Z","shell.execute_reply.started":"2026-01-21T12:58:20.359194Z","shell.execute_reply":"2026-01-21T12:58:21.294517Z"}},"outputs":[{"name":"stdout","text":"OK | Saved:\n - /kaggle/working/ecgdig/artifacts/train_base_folds.parquet\n - /kaggle/working/ecgdig/artifacts/train_manifest_folds.parquet\n - /kaggle/working/ecgdig/artifacts/train_img_table.parquet\n - /kaggle/working/ecgdig/artifacts/fold_stats.json\n - /kaggle/working/ecgdig/artifacts/mismatch_report.json\nUsed base_ids: 977 | Used variants: 8793\nFold base_ids_per_fold: {0: 196, 1: 196, 2: 195, 3: 195, 4: 195}\nMismatch (manifest not in train.csv): 0\nMismatch (train.csv not in manifest): 0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Page Geometry Normalization and Robust Lead Localization","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os, json, shutil\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\n# ============================================================\n# STAGE 3 — REVISED FULL (Option A + Parquet dtype-safe)\n# ============================================================\n\nWORK_ROOT = Path(\"/kaggle/working/ecgdig\")\nTMP_ROOT  = Path(\"/kaggle/temp/ecgdig_cache\")\n\nART_DIR   = WORK_ROOT / \"artifacts\"\nMODEL_DIR = WORK_ROOT / \"models\"\n\nCACHE_DIR = TMP_ROOT / \"cache\"\nNORM_DIR  = CACHE_DIR / \"norm\"\nCROP_DIR  = CACHE_DIR / \"crops\"\nYOLO_DIR  = TMP_ROOT / \"yolo_roi\"\nYOLO_RUNS = TMP_ROOT / \"yolo_runs\"\n\nfor d in [WORK_ROOT, ART_DIR, MODEL_DIR, TMP_ROOT, CACHE_DIR, NORM_DIR, CROP_DIR, YOLO_DIR, YOLO_RUNS]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# ---- load artifacts\ndf_train = pd.read_parquet(ART_DIR / \"train_img_table.parquet\")\ndf_testm = pd.read_parquet(ART_DIR / \"test_meta.parquet\")\ndf_test  = pd.read_parquet(ART_DIR / \"test_manifest.parquet\")\n\nassert {\"base_id\",\"variant_tag\",\"image_path\",\"fold\"}.issubset(df_train.columns)\nassert {\"base_id\",\"image_path\"}.issubset(df_test.columns)\nassert {\"id\",\"lead\",\"fs\",\"number_of_rows\"}.issubset(df_testm.columns)\n\n# ---- dtype normalize (CRITICAL for pyarrow)\ndf_train[\"base_id\"] = df_train[\"base_id\"].astype(str)\ndf_train[\"variant_tag\"] = df_train[\"variant_tag\"].astype(str)\ndf_train[\"image_path\"] = df_train[\"image_path\"].astype(str)\n\ndf_test[\"base_id\"] = df_test[\"base_id\"].astype(str)\ndf_test[\"image_path\"] = df_test[\"image_path\"].astype(str)\n\n# ---- build unified image table (train variants + test images)\ndf_tr_img = df_train[[\"base_id\",\"variant_tag\",\"image_path\",\"fold\"]].copy()\ndf_tr_img[\"split\"] = \"train\"\ndf_tr_img[\"uid\"] = df_tr_img[\"base_id\"] + \"__\" + df_tr_img[\"variant_tag\"]\n\ndf_te_img = df_test[[\"base_id\",\"image_path\"]].copy()\ndf_te_img[\"variant_tag\"] = \"test\"\ndf_te_img[\"fold\"] = -1\ndf_te_img[\"split\"] = \"test\"\ndf_te_img[\"uid\"] = df_te_img[\"base_id\"] + \"__test\"\n\ndf_img = pd.concat([df_tr_img, df_te_img], ignore_index=True)\ndf_img[\"norm_path\"] = df_img[\"uid\"].map(lambda u: str(NORM_DIR / f\"{u}.png\"))\n\n# ---- geometry normalization helpers\ndef _read_bgr(path: str) -> np.ndarray:\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    assert img is not None\n    return img\n\ndef _resize_max_side(img: np.ndarray, max_side: int) -> np.ndarray:\n    h, w = img.shape[:2]\n    s = min(1.0, float(max_side) / float(max(h, w)))\n    nw = int(round(w * s))\n    nh = int(round(h * s))\n    return cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n\ndef _estimate_deskew_angle(gray: np.ndarray) -> float:\n    e = cv2.Canny(gray, 50, 150)\n    lines = cv2.HoughLinesP(e, 1, np.pi/180.0, threshold=120, minLineLength=gray.shape[1]//3, maxLineGap=20)\n    try:\n        x1 = lines[:,0,0].astype(np.float32); y1 = lines[:,0,1].astype(np.float32)\n        x2 = lines[:,0,2].astype(np.float32); y2 = lines[:,0,3].astype(np.float32)\n        ang = np.degrees(np.arctan2((y2 - y1), (x2 - x1)))\n        ang = ((ang + 45.0) % 90.0) - 45.0\n        a = float(np.nanmedian(ang))\n    except Exception:\n        a = 0.0\n    return float(np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0))\n\ndef _rotate(img: np.ndarray, angle_deg: float) -> np.ndarray:\n    h, w = img.shape[:2]\n    M = cv2.getRotationMatrix2D((w/2.0, h/2.0), angle_deg, 1.0)\n    return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n\ndef _content_bbox(gray: np.ndarray) -> tuple:\n    g = cv2.GaussianBlur(gray, (3,3), 0)\n    _, thr = cv2.threshold(g, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    thr = cv2.morphologyEx(thr, cv2.MORPH_OPEN, np.ones((3,3), np.uint8), iterations=1)\n    ys, xs = np.where(thr > 0)\n    x1 = int(xs.min()); x2 = int(xs.max()) + 1\n    y1 = int(ys.min()); y2 = int(ys.max()) + 1\n    return x1, y1, x2, y2\n\ndef _normalize_one(src_path: str, max_side: int = 1600):\n    bgr = _read_bgr(src_path)\n    bgr = _resize_max_side(bgr, max_side=max_side)\n    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n    ang = _estimate_deskew_angle(gray)\n    rot = _rotate(bgr, -ang)\n    gray2 = cv2.cvtColor(rot, cv2.COLOR_BGR2GRAY)\n    try:\n        x1,y1,x2,y2 = _content_bbox(gray2)\n    except Exception:\n        h,w = gray2.shape[:2]\n        x1,y1,x2,y2 = 0,0,w,h\n    pad = int(round(0.01 * min((x2-x1), (y2-y1))))\n    x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n    x2 = min(rot.shape[1], x2 + pad); y2 = min(rot.shape[0], y2 + pad)\n    crop = rot[y1:y2, x1:x2].copy()\n    prm = {\"angle_deg\": float(ang),\n           \"content_bbox\": f\"{int(x1)},{int(y1)},{int(x2)},{int(y2)}\",\n           \"max_side\": int(max_side)}\n    return crop, prm\n\n# ---- template ROI boxes\nLEADS_12 = [\"I\",\"aVR\",\"V1\",\"V4\",\n            \"II\",\"aVL\",\"V2\",\"V5\",\n            \"III\",\"aVF\",\"V3\",\"V6\"]\nLEAD_LONG = \"II_LONG\"\nCLASSES = LEADS_12 + [LEAD_LONG]\nCLASS2ID = {c:i for i,c in enumerate(CLASSES)}\n\ndef _template_boxes(h: int, w: int):\n    pw = int(round(0.01 * w))\n    ph = int(round(0.01 * h))\n    y_main0 = 0\n    y_main1 = int(round(0.75 * h))\n    y_rhy0  = y_main1\n    y_rhy1  = h\n\n    boxes = []\n    cell_h = (y_main1 - y_main0) / 3.0\n    cell_w = w / 4.0\n    for idx, lead in enumerate(LEADS_12):\n        r = idx // 4\n        c = idx % 4\n        x1 = int(round(c * cell_w)) + pw\n        x2 = int(round((c+1) * cell_w)) - pw\n        y1 = int(round(y_main0 + r * cell_h)) + ph\n        y2 = int(round(y_main0 + (r+1) * cell_h)) - ph\n        boxes.append((lead, x1, y1, x2, y2))\n    boxes.append((LEAD_LONG, pw, y_rhy0 + ph, w - pw, y_rhy1 - ph))\n    return boxes\n\n# ---- normalize all images (train variants + test) -> /kaggle/temp\nnorm_params = []\nfor uid, src, dst in tqdm(df_img[[\"uid\",\"image_path\",\"norm_path\"]].itertuples(index=False), total=len(df_img)):\n    img_norm, prm = _normalize_one(src, max_side=1600)\n    cv2.imwrite(dst, img_norm)\n    prm.update({\"uid\": str(uid), \"norm_path\": str(dst), \"h\": int(img_norm.shape[0]), \"w\": int(img_norm.shape[1])})\n    norm_params.append(prm)\n\ndf_norm = pd.DataFrame(norm_params)\ndf_norm = df_norm.merge(df_img[[\"uid\",\"base_id\",\"variant_tag\",\"fold\",\"split\"]], on=\"uid\", how=\"left\")\n\n# ---- double-safe dtype for parquet\ndf_norm[\"base_id\"] = df_norm[\"base_id\"].astype(str)\ndf_norm[\"variant_tag\"] = df_norm[\"variant_tag\"].astype(str)\ndf_norm[\"split\"] = df_norm[\"split\"].astype(str)\ndf_norm[\"uid\"] = df_norm[\"uid\"].astype(str)\ndf_norm[\"norm_path\"] = df_norm[\"norm_path\"].astype(str)\n\ndf_norm.to_parquet(ART_DIR / \"norm_manifest.parquet\", index=False)\n\n# ---- build template ROI boxes + save crops -> /kaggle/temp\nroi_rows = []\nfor uid, base_id, split, norm_path, h, w in tqdm(df_norm[[\"uid\",\"base_id\",\"split\",\"norm_path\",\"h\",\"w\"]].itertuples(index=False), total=len(df_norm)):\n    img = _read_bgr(norm_path)\n    for lead, x1,y1,x2,y2 in _template_boxes(int(h), int(w)):\n        crop = img[y1:y2, x1:x2]\n        outp = CROP_DIR / f\"{uid}__{lead}.png\"\n        cv2.imwrite(str(outp), crop)\n        roi_rows.append((str(uid), str(base_id), str(split), str(lead), int(x1),int(y1),int(x2),int(y2), int(w),int(h), str(outp)))\n\ndf_roi_tpl = pd.DataFrame(roi_rows, columns=[\"uid\",\"base_id\",\"split\",\"lead\",\"x1\",\"y1\",\"x2\",\"y2\",\"img_w\",\"img_h\",\"crop_path\"])\ndf_roi_tpl[\"base_id\"] = df_roi_tpl[\"base_id\"].astype(str)\ndf_roi_tpl[\"uid\"] = df_roi_tpl[\"uid\"].astype(str)\ndf_roi_tpl[\"lead\"] = df_roi_tpl[\"lead\"].astype(str)\ndf_roi_tpl[\"crop_path\"] = df_roi_tpl[\"crop_path\"].astype(str)\n\ndf_roi_tpl.to_parquet(ART_DIR / \"roi_boxes_template.parquet\", index=False)\n\n# ---- export YOLO ROI dataset -> /kaggle/temp\nfor p in [\n    YOLO_DIR / \"images/train\", YOLO_DIR / \"images/val\",\n    YOLO_DIR / \"labels/train\", YOLO_DIR / \"labels/val\"\n]:\n    p.mkdir(parents=True, exist_ok=True)\n\ndf_yolo = df_norm[df_norm[\"split\"].eq(\"train\")].copy()\ndf_yolo[\"yolo_split\"] = np.where(df_yolo[\"fold\"].values == 0, \"val\", \"train\")\n\ndef _link_or_copy(src: str, dst: str):\n    try:\n        os.link(src, dst)\n    except Exception:\n        shutil.copy2(src, dst)\n\nfor uid, norm_path, ysplit, h, w in tqdm(df_yolo[[\"uid\",\"norm_path\",\"yolo_split\",\"h\",\"w\"]].itertuples(index=False), total=len(df_yolo)):\n    img_dst = YOLO_DIR / \"images\" / ysplit / f\"{uid}.png\"\n    lab_dst = YOLO_DIR / \"labels\" / ysplit / f\"{uid}.txt\"\n    _link_or_copy(str(norm_path), str(img_dst))\n\n    boxes = _template_boxes(int(h), int(w))\n    lines = []\n    for lead, x1,y1,x2,y2 in boxes:\n        cx = ((x1 + x2) / 2.0) / float(w)\n        cy = ((y1 + y2) / 2.0) / float(h)\n        bw = (x2 - x1) / float(w)\n        bh = (y2 - y1) / float(h)\n        lines.append(f\"{CLASS2ID[lead]} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n    lab_dst.write_text(\"\\n\".join(lines))\n\nyolo_yaml = YOLO_DIR / \"roi.yaml\"\nyolo_yaml.write_text(\n    \"path: \" + str(YOLO_DIR) + \"\\n\"\n    \"train: images/train\\n\"\n    \"val: images/val\\n\"\n    \"names:\\n\" + \"\\n\".join([f\"  {i}: {name}\" for i,name in enumerate(CLASSES)]) + \"\\n\"\n)\n\n# ---- optional YOLOv8n train\nyolo_ckpt = MODEL_DIR / \"yolo_roi.pt\"\ntrain_log = {\"trained\": False}\n\ntry:\n    from ultralytics import YOLO\n    y = YOLO(\"yolov8n.pt\")\n    r = y.train(\n        data=str(yolo_yaml),\n        epochs=15,\n        imgsz=960,\n        batch=16,\n        workers=2,\n        verbose=False,\n        project=str(YOLO_RUNS),\n        name=\"roi\"\n    )\n    best_pt = Path(r.save_dir) / \"weights\" / \"best.pt\"\n    shutil.copy2(best_pt, yolo_ckpt)\n    train_log = {\"trained\": True, \"best_pt\": str(best_pt), \"saved_as\": str(yolo_ckpt), \"save_dir\": str(r.save_dir)}\nexcept Exception as e:\n    train_log = {\"trained\": False, \"error\": repr(e), \"note\": \"Ultralytics not available or training failed; template ROIs already saved.\"}\n\n(Path(ART_DIR) / \"yolo_train_log.json\").write_text(json.dumps(train_log, indent=2))\n\n# ---- optional test-time refine ROIs\ntry:\n    from ultralytics import YOLO\n    y = YOLO(str(yolo_ckpt))\n\n    df_te = df_norm[df_norm[\"split\"].eq(\"test\")][[\"uid\",\"base_id\",\"norm_path\",\"h\",\"w\"]].copy()\n    refined_rows = []\n    for uid, base_id, norm_path, h, w in tqdm(df_te.itertuples(index=False), total=len(df_te)):\n        pred = y.predict(source=str(norm_path), imgsz=960, conf=0.10, iou=0.40, verbose=False)[0]\n        b = pred.boxes\n        cls = b.cls.cpu().numpy().astype(int)\n        xyxy = b.xyxy.cpu().numpy().astype(np.float32)\n        conf = b.conf.cpu().numpy().astype(np.float32)\n        d = pd.DataFrame({\"class_id\": cls, \"conf\": conf,\n                          \"x1\": xyxy[:,0], \"y1\": xyxy[:,1], \"x2\": xyxy[:,2], \"y2\": xyxy[:,3]})\n        d = d.sort_values([\"class_id\",\"conf\"]).drop_duplicates(\"class_id\", keep=\"last\")\n        d[\"lead\"] = d[\"class_id\"].map(lambda i: CLASSES[int(i)])\n        for lead, x1,y1,x2,y2 in d[[\"lead\",\"x1\",\"y1\",\"x2\",\"y2\"]].itertuples(index=False):\n            refined_rows.append((str(uid), str(lead), float(x1),float(y1),float(x2),float(y2)))\n\n    df_ref = pd.DataFrame(refined_rows, columns=[\"uid\",\"lead\",\"x1\",\"y1\",\"x2\",\"y2\"])\n    df_ref = df_ref.merge(df_te[[\"uid\",\"h\",\"w\"]], on=\"uid\", how=\"left\")\n\n    df_tpl = df_roi_tpl[df_roi_tpl[\"split\"].eq(\"test\")][[\"uid\",\"lead\",\"x1\",\"y1\",\"x2\",\"y2\",\"img_w\",\"img_h\"]].copy()\n    df_tpl = df_tpl.rename(columns={\"img_w\":\"w\",\"img_h\":\"h\"})\n\n    df_m = df_tpl.merge(df_ref, on=[\"uid\",\"lead\"], how=\"left\", suffixes=(\"_tpl\",\"_pred\"))\n    df_m[\"x1\"] = df_m[\"x1_pred\"].combine_first(df_m[\"x1_tpl\"])\n    df_m[\"y1\"] = df_m[\"y1_pred\"].combine_first(df_m[\"y1_tpl\"])\n    df_m[\"x2\"] = df_m[\"x2_pred\"].combine_first(df_m[\"x2_tpl\"])\n    df_m[\"y2\"] = df_m[\"y2_pred\"].combine_first(df_m[\"y2_tpl\"])\n    df_m[\"img_w\"] = df_m[\"w\"]\n    df_m[\"img_h\"] = df_m[\"h\"]\n\n    df_m = df_m[[\"uid\",\"lead\",\"x1\",\"y1\",\"x2\",\"y2\",\"img_w\",\"img_h\"]].copy()\n    df_m[[\"x1\",\"y1\",\"x2\",\"y2\"]] = df_m[[\"x1\",\"y1\",\"x2\",\"y2\"]].round().astype(int)\n\n    for uid, lead, x1,y1,x2,y2 in tqdm(df_m[[\"uid\",\"lead\",\"x1\",\"y1\",\"x2\",\"y2\"]].itertuples(index=False), total=len(df_m)):\n        img = _read_bgr(str(NORM_DIR / f\"{uid}.png\"))\n        crop = img[int(y1):int(y2), int(x1):int(x2)]\n        outp = CROP_DIR / f\"{uid}__{lead}.png\"\n        cv2.imwrite(str(outp), crop)\n\n    df_roi_test_ref = df_roi_tpl[df_roi_tpl[\"split\"].eq(\"test\")].drop(columns=[\"x1\",\"y1\",\"x2\",\"y2\"], errors=\"ignore\")\n    df_roi_test_ref = df_roi_test_ref.merge(df_m, on=[\"uid\",\"lead\"], how=\"left\")\n    df_roi_test_ref.to_parquet(ART_DIR / \"roi_boxes_test_refined.parquet\", index=False)\n\nexcept Exception as e:\n    (Path(ART_DIR) / \"yolo_refine_log.json\").write_text(json.dumps({\"refined\": False, \"error\": repr(e)}, indent=2))\n\nprint(\"OK | Saved artifacts (working):\")\nprint(\" -\", ART_DIR / \"norm_manifest.parquet\")\nprint(\" -\", ART_DIR / \"roi_boxes_template.parquet\")\nprint(\" -\", ART_DIR / \"yolo_train_log.json\")\nprint(\" -\", yolo_ckpt)\nprint(\"Temp caches (not counted in commit output):\")\nprint(\" -\", NORM_DIR)\nprint(\" -\", CROP_DIR)\nprint(\" -\", YOLO_DIR)\nprint(\" -\", YOLO_RUNS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T12:58:21.300286Z","iopub.execute_input":"2026-01-21T12:58:21.300516Z","iopub.status.idle":"2026-01-21T15:12:37.793594Z","shell.execute_reply.started":"2026-01-21T12:58:21.300493Z","shell.execute_reply":"2026-01-21T15:12:37.792482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8817 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5348d83f21c846e2889bb62c0386d262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9081 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ab97d0c79842e9888808d302cea718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8793 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ac7ed99894482ea2b4ff3092dd0396"}},"metadata":{}},{"name":"stdout","text":"OK | Saved artifacts (working):\n - /kaggle/working/ecgdig/artifacts/norm_manifest.parquet\n - /kaggle/working/ecgdig/artifacts/roi_boxes_template.parquet\n - /kaggle/working/ecgdig/artifacts/yolo_train_log.json\n - /kaggle/working/ecgdig/models/yolo_roi.pt\nTemp caches (not counted in commit output):\n - /kaggle/temp/ecgdig_cache/cache/norm\n - /kaggle/temp/ecgdig_cache/cache/crops\n - /kaggle/temp/ecgdig_cache/yolo_roi\n - /kaggle/temp/ecgdig_cache/yolo_runs\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Pseudo-Label Mask Generation and Segmentation Training","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json, math, random\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import SegformerForSemanticSegmentation\n\n# ============================================================\n# STAGE 4 — Pseudo-Label Mask Generation + SegFormer-B1 Training (REVISED FULL, SVD-safe)\n# ============================================================\n\n# Config\nOUT_ROOT  = Path(\"/kaggle/working/ecgdig\")\nART_DIR   = OUT_ROOT / \"artifacts\"\nMODEL_DIR = OUT_ROOT / \"models\"\nMASK_DBG_DIR = OUT_ROOT / \"debug\" / \"pseudo_masks\"\nfor d in [MODEL_DIR, MASK_DBG_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nFOLD = 0\nIMG_SIZE = 512\nBATCH = 8\nEPOCHS = 6\nLR = 2e-4\nWD = 1e-2\nNUM_WORKERS = 2\nSEED = 42\n\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nLEADS_12 = [\"I\",\"aVR\",\"V1\",\"V4\",\"II\",\"aVL\",\"V2\",\"V5\",\"III\",\"aVF\",\"V3\",\"V6\"]\nCLASSES  = LEADS_12 + [\"II_LONG\"]\nLEAD_SEC = {**{k: 2.5 for k in LEADS_12}, \"II_LONG\": 10.0}\nLEAD_COL = {**{k: k for k in LEADS_12}, \"II_LONG\": \"II\"}\n\nGT_ROOT = Path(\"/kaggle/input/physionet-ecg-image-digitization\") / \"train\"\n\n# ---- load stage-3 artifacts (crops) + folds\ndf_roi = pd.read_parquet(ART_DIR / \"roi_boxes_template.parquet\")\ndf_folds = pd.read_parquet(ART_DIR / \"train_base_folds.parquet\")[[\"base_id\",\"fold\",\"fs\",\"sig_len\"]]\n\ndf_roi[\"base_id\"] = df_roi[\"base_id\"].astype(str)\ndf_roi[\"lead\"] = df_roi[\"lead\"].astype(str)\ndf_folds[\"base_id\"] = df_folds[\"base_id\"].astype(str)\n\ndf_roi = df_roi[df_roi[\"split\"].eq(\"train\")].merge(df_folds, on=\"base_id\", how=\"inner\")\ndf_roi = df_roi[df_roi[\"lead\"].isin(CLASSES)].reset_index(drop=True)\n\ndf_tr = df_roi[df_roi[\"fold\"].ne(FOLD)].reset_index(drop=True)\ndf_va = df_roi[df_roi[\"fold\"].eq(FOLD)].reset_index(drop=True)\n\nseg_table = {\n    \"FOLD\": int(FOLD),\n    \"IMG_SIZE\": int(IMG_SIZE),\n    \"BATCH\": int(BATCH),\n    \"EPOCHS\": int(EPOCHS),\n    \"LR\": float(LR),\n    \"WD\": float(WD),\n    \"N_TRAIN\": int(len(df_tr)),\n    \"N_VALID\": int(len(df_va)),\n}\n(Path(ART_DIR) / \"seg_table_info.json\").write_text(json.dumps(seg_table, indent=2))\ndf_tr.to_parquet(ART_DIR / f\"seg_train_table_fold{FOLD}.parquet\", index=False)\ndf_va.to_parquet(ART_DIR / f\"seg_valid_table_fold{FOLD}.parquet\", index=False)\n\n# ---- augmentations\naug_tr = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REPLICATE),\n    A.OneOf([\n        A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n        A.MotionBlur(blur_limit=7, p=1.0),\n    ], p=0.35),\n    A.OneOf([\n        A.RandomBrightnessContrast(0.2, 0.2, p=1.0),\n        A.RandomGamma(gamma_limit=(70, 140), p=1.0),\n    ], p=0.5),\n    A.Perspective(scale=(0.02, 0.06), keep_size=True, p=0.25),\n    A.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.06, rotate_limit=3, border_mode=cv2.BORDER_REPLICATE, p=0.35),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(transpose_mask=True),\n])\n\naug_va = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REPLICATE),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(transpose_mask=True),\n])\n\n# ---- stable linear fit (no SVD): y ≈ b - a*vn  with a>=0\ndef _fit_b_a(vn: np.ndarray, y: np.ndarray):\n    # x2 = -vn, fit y = b + c*x2, enforce c>=0, then y = b - c*vn\n    x2 = (-vn).astype(np.float32)\n    y  = y.astype(np.float32)\n\n    m = (np.isfinite(x2) & np.isfinite(y)).astype(np.float32)\n    cnt = float(m.sum()) + 1e-6\n\n    mx = float((x2 * m).sum() / cnt)\n    my = float((y  * m).sum() / cnt)\n\n    dx = (x2 - mx)\n    dy = (y  - my)\n\n    varx = float(((dx*dx) * m).sum() / cnt) + 1e-6\n    cov  = float(((dx*dy) * m).sum() / cnt)\n\n    c = float(np.abs(cov / varx))  # enforce positive without if/else\n    b = float(my - c * mx)\n    return b, c\n\n# Dataset: pseudo-mask from GT aligned to ink (SVD-safe)\nclass ECGSegDS(Dataset):\n    def __init__(self, df: pd.DataFrame, is_train: bool):\n        self.df = df.reset_index(drop=True)\n        self.tf = aug_tr if is_train else aug_va\n        self.gt_cache = {}\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i: int):\n        r = self.df.iloc[i]\n        crop_path = str(r[\"crop_path\"])\n        base_id = str(r[\"base_id\"])\n        lead = str(r[\"lead\"])\n        fs = float(r[\"fs\"])\n\n        # cache GT per base_id\n        if base_id not in self.gt_cache:\n            gt_csv = GT_ROOT / base_id / f\"{base_id}.csv\"\n            self.gt_cache[base_id] = pd.read_csv(gt_csv)\n        sig_df = self.gt_cache[base_id]\n\n        col = LEAD_COL[lead]\n        v_full = sig_df[col].values.astype(np.float32)\n        v_full = np.nan_to_num(v_full, nan=0.0, posinf=0.0, neginf=0.0)\n\n        n = int(math.floor(fs * LEAD_SEC[lead]))\n        n = int(min(n, v_full.shape[0]))\n        v = v_full[:n]\n\n        bgr = cv2.imread(crop_path, cv2.IMREAD_COLOR)\n        assert bgr is not None\n        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY).astype(np.uint8)\n\n        # ink map\n        ink = (255 - gray).astype(np.float32)\n        ink = cv2.GaussianBlur(ink, (3,3), 0)\n\n        h0, w0 = ink.shape[:2]\n\n        # resample GT to crop width\n        x0 = np.linspace(0.0, 1.0, num=w0, dtype=np.float32)\n        xv = np.linspace(0.0, 1.0, num=max(1, len(v)), dtype=np.float32)\n        v_w = np.interp(x0, xv, v).astype(np.float32)\n        v_w = np.nan_to_num(v_w, nan=0.0, posinf=0.0, neginf=0.0)\n\n        med = float(np.median(v_w))\n        p05, p95 = np.nanpercentile(v_w, [5, 95]).astype(np.float32)\n        scale = float((p95 - p05) + 1e-6)\n        vn = (v_w - med) / scale\n        vn = np.nan_to_num(vn, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\n        # soft y from ink (more stable than argmax)\n        yy = np.arange(h0, dtype=np.float32).reshape(-1, 1)\n        w = ink + 1e-3\n        y_soft = (w * yy).sum(axis=0) / (w.sum(axis=0) + 1e-6)\n        y_soft = np.nan_to_num(y_soft, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\n        # fit affine\n        b, a = _fit_b_a(vn, y_soft)\n\n        y = (b - a * vn).astype(np.int32)\n        y = np.clip(y, 0, h0 - 1)\n\n        # render polyline mask\n        m = np.zeros((h0, w0), np.uint8)\n        pts = np.stack([np.arange(w0, dtype=np.int32), y], axis=1).reshape(-1,1,2)\n        cv2.polylines(m, [pts], isClosed=False, color=255, thickness=2, lineType=cv2.LINE_AA)\n        m = cv2.morphologyEx(m, cv2.MORPH_DILATE, np.ones((3,3), np.uint8), iterations=1)\n\n        out = self.tf(image=bgr, mask=m)\n        img_t = out[\"image\"]\n        m_t = out[\"mask\"].float().unsqueeze(0) / 255.0\n        return img_t, m_t, base_id, lead\n\nds_tr = ECGSegDS(df_tr, is_train=True)\nds_va = ECGSegDS(df_va, is_train=False)\n\ndl_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS,\n                   pin_memory=True, drop_last=True)\ndl_va = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS,\n                   pin_memory=True, drop_last=False)\n\n# Model: SegFormer-B1 (binary trace)\nMODEL_NAME = \"nvidia/segformer-b1-finetuned-ade-512-512\"\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    MODEL_NAME,\n    num_labels=1,\n    ignore_mismatched_sizes=True,\n)\nmodel.to(device)\n\nopt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\nbce = nn.BCEWithLogitsLoss()\n\nscaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n\ndef _run_epoch(dl, train: bool):\n    model.train(mode=train)\n    losses = []\n    for img, m, _, _ in tqdm(dl, leave=False):\n        img = img.to(device, non_blocking=True)\n        m   = m.to(device, non_blocking=True)\n\n        with torch.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n            out = model(pixel_values=img)\n            logits = out.logits\n            logits = F.interpolate(logits, size=m.shape[-2:], mode=\"bilinear\", align_corners=False)\n            loss = bce(logits, m)\n\n        if train:\n            opt.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        losses.append(float(loss.detach().cpu().item()))\n    return float(np.mean(losses)) if len(losses) else float(\"nan\")\n\nbest_va = 1e9\nhist = []\nfor ep in range(EPOCHS):\n    tr_loss = _run_epoch(dl_tr, train=True)\n    va_loss = _run_epoch(dl_va, train=False)\n    hist.append({\"epoch\": ep+1, \"train_loss\": tr_loss, \"valid_loss\": va_loss})\n    best_va = min(best_va, va_loss)\n    print(f\"Epoch {ep+1}/{EPOCHS} | train_loss={tr_loss:.5f} | valid_loss={va_loss:.5f}\")\n\n# Save model\nsave_path = MODEL_DIR / f\"segformer_b1_fold{FOLD}.pt\"\ntorch.save(\n    {\n        \"model_name\": MODEL_NAME,\n        \"state_dict\": model.state_dict(),\n        \"fold\": int(FOLD),\n        \"img_size\": int(IMG_SIZE),\n        \"classes\": [\"trace\"],\n        \"leads\": CLASSES,\n        \"train_cfg\": {\"batch\": BATCH, \"epochs\": EPOCHS, \"lr\": LR, \"wd\": WD, \"seed\": SEED},\n        \"history\": hist,\n    },\n    save_path\n)\n(Path(ART_DIR) / f\"segformer_b1_fold{FOLD}_history.json\").write_text(json.dumps(hist, indent=2))\n\n# quick debug dump (first 12 samples from valid)\ndbg_idx = np.arange(min(12, len(df_va)))\nfor j in dbg_idx:\n    img_t, m_t, bid, lead = ds_va[int(j)]\n    img = img_t.permute(1,2,0).cpu().numpy()\n    img = (img * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406]))\n    img = np.clip(img*255.0, 0, 255).astype(np.uint8)\n    msk = (m_t.squeeze(0).cpu().numpy()*255.0).astype(np.uint8)\n    cv2.imwrite(str(MASK_DBG_DIR / f\"va_{j:03d}__{bid}__{lead}__img.png\"), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n    cv2.imwrite(str(MASK_DBG_DIR / f\"va_{j:03d}__{bid}__{lead}__mask.png\"), msk)\n\nprint(\"OK | Saved:\")\nprint(\" -\", ART_DIR / f\"seg_train_table_fold{FOLD}.parquet\")\nprint(\" -\", ART_DIR / f\"seg_valid_table_fold{FOLD}.parquet\")\nprint(\" -\", save_path)\nprint(\" -\", MASK_DBG_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T15:12:37.794981Z","iopub.execute_input":"2026-01-21T15:12:37.795299Z","execution_failed":"2026-01-21T22:22:14.170Z"}},"outputs":[{"name":"stderr","text":"2026-01-21 15:12:55.656009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769008375.866964      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769008375.929732      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769008376.459837      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769008376.459886      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769008376.459889      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769008376.459892      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ebf1d9f690243e6beaa63a8349aedd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/54.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6db0b2eb114b45d096ee32fcab8faea1"}},"metadata":{}},{"name":"stderr","text":"Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b1-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11422 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2162202e55fb43e88c4846676593b229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/54.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad191ab84984aa08207e47fb756213f"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Prob Map Inference, DP/Viterbi Vectorization, and Signal Extraction","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom transformers import SegformerForSemanticSegmentation\n\n# Config\nOUT_ROOT  = Path(\"/kaggle/working/ecgdig\")\nART_DIR   = OUT_ROOT / \"artifacts\"\nMODEL_DIR = OUT_ROOT / \"models\"\nCACHE_DIR = OUT_ROOT / \"cache\"\nPROB_DIR  = CACHE_DIR / \"prob_u8\"\nTRACE_DIR = CACHE_DIR / \"trace_px\"\nfor d in [PROB_DIR, TRACE_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nFOLD = 0\nIMG_SIZE = 512\nBATCH = 24\nNUM_WORKERS = 2\n\n# DP params (fast + stable)\nDP_H_MAX = 256\nDP_W_MAX = 512\nMAX_DY = 12\nLAMBDA = 0.65\nEPS = 1e-6\n\nSAVE_PROB_U8 = 1  # keep as 1 for debugging/QA; u8 is compact\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load ROI table (prefer refined test if exists)\ndf_roi = pd.read_parquet(ART_DIR / \"roi_boxes_template.parquet\")\ntry:\n    df_ref = pd.read_parquet(ART_DIR / \"roi_boxes_test_refined.parquet\")\n    df_roi = pd.concat([df_roi[df_roi[\"split\"].ne(\"test\")], df_ref], ignore_index=True)\nexcept Exception:\n    pass\n\ndf_roi = df_roi[df_roi[\"split\"].eq(\"test\")].reset_index(drop=True)\nassert {\"uid\",\"base_id\",\"lead\",\"crop_path\",\"split\"}.issubset(df_roi.columns)\nassert len(df_roi) > 0\n\n\n# Inference transform\ntf_inf = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REPLICATE),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(),\n])\n\nclass CropDS(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df.reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i: int):\n        r = self.df.iloc[i]\n        p = r[\"crop_path\"]\n        bgr = cv2.imread(p, cv2.IMREAD_COLOR)\n        assert bgr is not None\n        h0, w0 = bgr.shape[:2]\n        out = tf_inf(image=bgr)\n        img = out[\"image\"]\n        return img, int(h0), int(w0), str(r[\"uid\"]), str(r[\"base_id\"]), str(r[\"lead\"]), str(p)\n\ndl = DataLoader(CropDS(df_roi), batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n\n# Load SegFormer-B1 checkpoint\nckpt_path = MODEL_DIR / f\"segformer_b1_fold{FOLD}.pt\"\nassert ckpt_path.exists()\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\nMODEL_NAME = ckpt[\"model_name\"]\n\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    MODEL_NAME,\n    num_labels=1,\n    ignore_mismatched_sizes=True,\n)\nmodel.load_state_dict(ckpt[\"state_dict\"], strict=False)\nmodel.to(device).eval()\n\n\n# DP/Viterbi seam (windowed L1 transitions; fast)\ndef viterbi_seam(prob_hw: np.ndarray, lam: float, max_dy: int) -> np.ndarray:\n    H, W = prob_hw.shape\n    cost = (-np.log(prob_hw + EPS)).astype(np.float32)\n    dp = cost[:, 0].copy()\n    back = np.empty((H, W), dtype=np.int16)\n    back[:, 0] = np.arange(H, dtype=np.int16)\n\n    y_idx = np.arange(H, dtype=np.int16)\n    dp_pad = None\n    shifts = np.arange(-max_dy, max_dy + 1, dtype=np.int16)\n\n    for x in range(1, W):\n        dp_pad = np.pad(dp, (max_dy, max_dy), mode=\"constant\", constant_values=np.inf)\n        best = np.full(H, np.inf, dtype=np.float32)\n        best_prev = np.zeros(H, dtype=np.int16)\n\n        for s in shifts:\n            prev = dp_pad[(max_dy - int(s)):(max_dy - int(s) + H)]\n            cand = prev + (lam * float(abs(int(s))))\n            m = cand < best\n            best = np.where(m, cand, best)\n            best_prev = np.where(m, (y_idx - int(s)).astype(np.int16), best_prev)\n\n        dp = cost[:, x] + best\n        back[:, x] = best_prev\n\n    y = int(np.argmin(dp))\n    path = np.empty(W, dtype=np.int16)\n    path[W - 1] = y\n    for x in range(W - 1, 0, -1):\n        y = int(back[y, x])\n        path[x - 1] = y\n    return path\n\n# Inference + save prob_u8 + save trace_px\nrows = []\nwith torch.no_grad():\n    for imgs, h0s, w0s, uids, base_ids, leads, crop_paths in tqdm(dl, total=len(dl)):\n        imgs = imgs.to(device, non_blocking=True)\n        out = model(pixel_values=imgs)\n        logits = out.logits\n        logits = F.interpolate(logits, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n        prob = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy().astype(np.float32)  # [B,512,512]\n\n        for k in range(prob.shape[0]):\n            h0 = int(h0s[k]); w0 = int(w0s[k])\n            uid = str(uids[k]); base_id = str(base_ids[k]); lead = str(leads[k]); crop_path = str(crop_paths[k])\n\n            pr = prob[k]\n            pr0 = cv2.resize(pr, (w0, h0), interpolation=cv2.INTER_LINEAR)\n\n            # optional prob_u8 save (compact)\n            prob_path = str(PROB_DIR / f\"{uid}__{lead}.npz\")\n            pr_u8 = np.clip(np.round(pr0 * 255.0), 0, 255).astype(np.uint8)\n            np.savez_compressed(prob_path, prob_u8=pr_u8, h=int(h0), w=int(w0))\n\n            # DP on downsampled prob for speed\n            Hd = int(max(2, min(DP_H_MAX, h0)))\n            Wd = int(max(2, min(DP_W_MAX, w0)))\n            pr_ds = cv2.resize(pr0, (Wd, Hd), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n            seam_ds = viterbi_seam(pr_ds, lam=LAMBDA, max_dy=MAX_DY).astype(np.float32)  # [Wd]\n            x_ds = np.arange(Wd, dtype=np.float32)\n            x0 = np.linspace(0.0, float(Wd - 1), num=w0, dtype=np.float32)\n            seam_x = np.interp(x0, x_ds, seam_ds).astype(np.float32)\n\n            scale_y = (float(h0 - 1) / float(max(1, Hd - 1)))\n            y0 = np.clip(seam_x * scale_y, 0.0, float(h0 - 1)).astype(np.float32)\n\n            trace_path = str(TRACE_DIR / f\"{uid}__{lead}.npy\")\n            np.save(trace_path, y0)\n\n            # confidence along seam (proxy)\n            xi = np.clip(np.round(np.arange(w0)).astype(np.int32), 0, w0 - 1)\n            yi = np.clip(np.round(y0).astype(np.int32), 0, h0 - 1)\n            conf = float(np.mean(pr0[yi, xi]))\n\n            rows.append((uid, base_id, lead, crop_path, prob_path, trace_path, int(h0), int(w0), int(Hd), int(Wd), conf))\n\ndf_out = pd.DataFrame(\n    rows,\n    columns=[\"uid\",\"base_id\",\"lead\",\"crop_path\",\"prob_u8_path\",\"trace_px_path\",\"h\",\"w\",\"dp_h\",\"dp_w\",\"mean_trace_conf\"]\n)\n\ndf_out.to_parquet(ART_DIR / f\"test_trace_index_fold{FOLD}.parquet\", index=False)\n(Path(ART_DIR) / f\"stage5_params_fold{FOLD}.json\").write_text(json.dumps({\n    \"fold\": int(FOLD),\n    \"img_size\": int(IMG_SIZE),\n    \"batch\": int(BATCH),\n    \"dp_h_max\": int(DP_H_MAX),\n    \"dp_w_max\": int(DP_W_MAX),\n    \"max_dy\": int(MAX_DY),\n    \"lambda\": float(LAMBDA),\n    \"save_prob_u8\": int(SAVE_PROB_U8),\n    \"n_rows\": int(len(df_out)),\n}, indent=2))\n\nprint(\"OK | Saved:\")\nprint(\" -\", ART_DIR / f\"test_trace_index_fold{FOLD}.parquet\")\nprint(\" -\", ART_DIR / f\"stage5_params_fold{FOLD}.json\")\nprint(\"Cache dirs:\")\nprint(\" -\", PROB_DIR)\nprint(\" -\", TRACE_DIR)\nprint(\"Rows:\", len(df_out), \"| mean conf:\", float(df_out[\"mean_trace_conf\"].mean()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prob Map Inference, DP/Viterbi Vectorization, and Signal Extraction","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom transformers import SegformerForSemanticSegmentation\n\n# Config\nOUT_ROOT  = Path(\"/kaggle/working/ecgdig\")\nART_DIR   = OUT_ROOT / \"artifacts\"\nMODEL_DIR = OUT_ROOT / \"models\"\nCACHE_DIR = OUT_ROOT / \"cache\"\nPROB_DIR  = CACHE_DIR / \"prob_u8\"\nTRACE_DIR = CACHE_DIR / \"trace_px\"\nfor d in [PROB_DIR, TRACE_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nFOLD = 0\nIMG_SIZE = 512\nBATCH = 24\nNUM_WORKERS = 2\n\n# DP params (fast + stable)\nDP_H_MAX = 256\nDP_W_MAX = 512\nMAX_DY = 12\nLAMBDA = 0.65\nEPS = 1e-6\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load ROI table (train+test). Prefer refined test if available.\ndf_roi_all = pd.read_parquet(ART_DIR / \"roi_boxes_template.parquet\")\ndf_roi_all[\"uid\"] = df_roi_all[\"uid\"].astype(str)\ndf_roi_all[\"base_id\"] = df_roi_all[\"base_id\"].astype(str)\ndf_roi_all[\"lead\"] = df_roi_all[\"lead\"].astype(str)\ndf_roi_all[\"crop_path\"] = df_roi_all[\"crop_path\"].astype(str)\n\ntry:\n    df_ref = pd.read_parquet(ART_DIR / \"roi_boxes_test_refined.parquet\")\n    df_ref[\"uid\"] = df_ref[\"uid\"].astype(str)\n    df_ref[\"base_id\"] = df_ref[\"base_id\"].astype(str)\n    df_ref[\"lead\"] = df_ref[\"lead\"].astype(str)\n    df_ref[\"crop_path\"] = df_ref[\"crop_path\"].astype(str)\n    df_roi_all = pd.concat([df_roi_all[df_roi_all[\"split\"].ne(\"test\")], df_ref], ignore_index=True)\nexcept Exception:\n    pass\n\n# Attach fold to train rows for OOF subset\ndf_folds = pd.read_parquet(ART_DIR / \"train_base_folds.parquet\")[[\"base_id\",\"fold\"]].copy()\ndf_folds[\"base_id\"] = df_folds[\"base_id\"].astype(str)\ndf_roi_all = df_roi_all.merge(df_folds, on=\"base_id\", how=\"left\")\n\n# Build run set: TEST + OOF(valid fold only)\ndf_test = df_roi_all[df_roi_all[\"split\"].eq(\"test\")].copy()\ndf_oof  = df_roi_all[df_roi_all[\"split\"].eq(\"train\") & df_roi_all[\"fold\"].eq(FOLD)].copy()\n\ndf_test[\"run_split\"] = \"test\"\ndf_oof[\"run_split\"]  = \"oof\"\n\ndf_run = pd.concat([df_test, df_oof], ignore_index=True).reset_index(drop=True)\nassert len(df_run) > 0\nassert {\"uid\",\"base_id\",\"lead\",\"crop_path\",\"run_split\"}.issubset(df_run.columns)\n\n\n# Inference transform\ntf_inf = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REPLICATE),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(),\n])\n\nclass CropDS(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df.reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i: int):\n        r = self.df.iloc[i]\n        p = r[\"crop_path\"]\n        bgr = cv2.imread(p, cv2.IMREAD_COLOR)\n        assert bgr is not None\n        h0, w0 = bgr.shape[:2]\n        out = tf_inf(image=bgr)\n        img = out[\"image\"]\n        return img, int(h0), int(w0), str(r[\"uid\"]), str(r[\"base_id\"]), str(r[\"lead\"]), str(r[\"run_split\"]), str(p)\n\ndl = DataLoader(CropDS(df_run), batch_size=BATCH, shuffle=False,\n                num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n\n# Load SegFormer-B1 checkpoint\nckpt_path = MODEL_DIR / f\"segformer_b1_fold{FOLD}.pt\"\nassert ckpt_path.exists()\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\nMODEL_NAME = ckpt[\"model_name\"]\n\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    MODEL_NAME,\n    num_labels=1,\n    ignore_mismatched_sizes=True,\n)\nmodel.load_state_dict(ckpt[\"state_dict\"], strict=False)\nmodel.to(device).eval()\n\n# DP/Viterbi seam (fast windowed transitions)\ndef viterbi_seam(prob_hw: np.ndarray, lam: float, max_dy: int) -> np.ndarray:\n    H, W = prob_hw.shape\n    cost = (-np.log(prob_hw + EPS)).astype(np.float32)\n    dp = cost[:, 0].copy()\n    back = np.empty((H, W), dtype=np.int16)\n    back[:, 0] = np.arange(H, dtype=np.int16)\n\n    y_idx = np.arange(H, dtype=np.int16)\n    shifts = np.arange(-max_dy, max_dy + 1, dtype=np.int16)\n\n    for x in range(1, W):\n        dp_pad = np.pad(dp, (max_dy, max_dy), mode=\"constant\", constant_values=np.inf)\n        best = np.full(H, np.inf, dtype=np.float32)\n        best_prev = np.zeros(H, dtype=np.int16)\n\n        for s in shifts:\n            prev = dp_pad[(max_dy - int(s)):(max_dy - int(s) + H)]\n            cand = prev + (lam * float(abs(int(s))))\n            m = cand < best\n            best = np.where(m, cand, best)\n            best_prev = np.where(m, (y_idx - int(s)).astype(np.int16), best_prev)\n\n        dp = cost[:, x] + best\n        back[:, x] = best_prev\n\n    y = int(np.argmin(dp))\n    path = np.empty(W, dtype=np.int16)\n    path[W - 1] = y\n    for x in range(W - 1, 0, -1):\n        y = int(back[y, x])\n        path[x - 1] = y\n    return path\n\n# Inference + save prob_u8 + save trace_px\nrows = []\nwith torch.no_grad():\n    for imgs, h0s, w0s, uids, base_ids, leads, run_splits, crop_paths in tqdm(dl, total=len(dl)):\n        imgs = imgs.to(device, non_blocking=True)\n        out = model(pixel_values=imgs)\n        logits = out.logits\n        logits = F.interpolate(logits, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n        prob = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy().astype(np.float32)  # [B,512,512]\n\n        for k in range(prob.shape[0]):\n            h0 = int(h0s[k]); w0 = int(w0s[k])\n            uid = str(uids[k]); base_id = str(base_ids[k]); lead = str(leads[k])\n            run_split = str(run_splits[k]); crop_path = str(crop_paths[k])\n\n            pr = prob[k]\n            pr0 = cv2.resize(pr, (w0, h0), interpolation=cv2.INTER_LINEAR)\n\n            # prob_u8 save (compact + deterministic)\n            prob_path = str(PROB_DIR / f\"{run_split}__{uid}__{lead}.npz\")\n            pr_u8 = np.clip(np.round(pr0 * 255.0), 0, 255).astype(np.uint8)\n            np.savez_compressed(prob_path, prob_u8=pr_u8, h=int(h0), w=int(w0))\n\n            # DP on downsampled prob for speed\n            Hd = int(max(2, min(DP_H_MAX, h0)))\n            Wd = int(max(2, min(DP_W_MAX, w0)))\n            pr_ds = cv2.resize(pr0, (Wd, Hd), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n\n            seam_ds = viterbi_seam(pr_ds, lam=LAMBDA, max_dy=MAX_DY).astype(np.float32)  # [Wd]\n            x_ds = np.arange(Wd, dtype=np.float32)\n            x0 = np.linspace(0.0, float(Wd - 1), num=w0, dtype=np.float32)\n            seam_x = np.interp(x0, x_ds, seam_ds).astype(np.float32)\n\n            scale_y = (float(h0 - 1) / float(max(1, Hd - 1)))\n            y0 = np.clip(seam_x * scale_y, 0.0, float(h0 - 1)).astype(np.float32)\n\n            trace_path = str(TRACE_DIR / f\"{run_split}__{uid}__{lead}.npy\")\n            np.save(trace_path, y0)\n\n            xi = np.arange(w0, dtype=np.int32)\n            yi = np.clip(np.round(y0).astype(np.int32), 0, h0 - 1)\n            conf = float(np.mean(pr0[yi, xi]))\n\n            rows.append((run_split, uid, base_id, lead, crop_path, prob_path, trace_path,\n                         int(h0), int(w0), int(Hd), int(Wd), conf))\n\ndf_out = pd.DataFrame(\n    rows,\n    columns=[\"run_split\",\"uid\",\"base_id\",\"lead\",\"crop_path\",\"prob_u8_path\",\"trace_px_path\",\n             \"h\",\"w\",\"dp_h\",\"dp_w\",\"mean_trace_conf\"]\n)\n\ndf_test_out = df_out[df_out[\"run_split\"].eq(\"test\")].reset_index(drop=True)\ndf_oof_out  = df_out[df_out[\"run_split\"].eq(\"oof\")].reset_index(drop=True)\n\ndf_test_out.to_parquet(ART_DIR / f\"test_trace_index_fold{FOLD}.parquet\", index=False)\ndf_oof_out.to_parquet(ART_DIR / f\"oof_trace_index_fold{FOLD}.parquet\", index=False)\n\n(Path(ART_DIR) / f\"stage5_params_fold{FOLD}.json\").write_text(json.dumps({\n    \"fold\": int(FOLD),\n    \"img_size\": int(IMG_SIZE),\n    \"batch\": int(BATCH),\n    \"dp_h_max\": int(DP_H_MAX),\n    \"dp_w_max\": int(DP_W_MAX),\n    \"max_dy\": int(MAX_DY),\n    \"lambda\": float(LAMBDA),\n    \"n_test_rows\": int(len(df_test_out)),\n    \"n_oof_rows\": int(len(df_oof_out)),\n}, indent=2))\n\nprint(\"OK | Saved:\")\nprint(\" -\", ART_DIR / f\"test_trace_index_fold{FOLD}.parquet\")\nprint(\" -\", ART_DIR / f\"oof_trace_index_fold{FOLD}.parquet\")\nprint(\" -\", ART_DIR / f\"stage5_params_fold{FOLD}.json\")\nprint(\"Cache dirs:\")\nprint(\" -\", PROB_DIR)3\nprint(\" -\", TRACE_DIR)\nprint(\"Rows | test:\", len(df_test_out), \"| oof:\", len(df_oof_out))\nprint(\"Mean conf | test:\", float(df_test_out[\"mean_trace_conf\"].mean()) if len(df_test_out) else float(\"nan\"))\nprint(\"Mean conf | oof :\", float(df_oof_out[\"mean_trace_conf\"].mean()) if len(df_oof_out) else float(\"nan\"))","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-21T22:22:14.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble, Submission Build, and Submission QA","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nOUT_ROOT  = Path(\"/kaggle/working/ecgdig\")\nART_DIR   = OUT_ROOT / \"artifacts\"\nCACHE_DIR = OUT_ROOT / \"cache\"\nSUB_DIR   = OUT_ROOT / \"subs\"\nSUB_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- load template + parse id\ndf_sub = pd.read_parquet(ART_DIR / \"sample_submission.parquet\")[[\"id\"]].copy()\ntmp = df_sub[\"id\"].astype(str).str.rsplit(\"_\", n=2, expand=True)\ndf_sub[\"base_id\"] = tmp[0].astype(str)\ndf_sub[\"row_id\"]  = tmp[1].astype(np.int32)\ndf_sub[\"lead\"]    = tmp[2].astype(str)\ndf_sub[\"key\"]     = df_sub[\"base_id\"] + \"|\" + df_sub[\"lead\"]\n\n# ---- load ROI table (for crop_path) and keep TEST only; prefer refined if exists\ndf_roi = pd.read_parquet(ART_DIR / \"roi_boxes_template.parquet\")\ntry:\n    df_ref = pd.read_parquet(ART_DIR / \"roi_boxes_test_refined.parquet\")\n    df_roi = pd.concat([df_roi[df_roi[\"split\"].ne(\"test\")], df_ref], ignore_index=True)\nexcept Exception:\n    pass\ndf_roi = df_roi[df_roi[\"split\"].eq(\"test\")][[\"uid\",\"base_id\",\"lead\",\"crop_path\"]].copy()\ndf_roi[\"base_id\"] = df_roi[\"base_id\"].astype(str)\ndf_roi[\"lead\"] = df_roi[\"lead\"].astype(str)\ndf_roi[\"crop_path\"] = df_roi[\"crop_path\"].astype(str)\n\n# ---- load all available Stage-5 test indexes (fold ensemble)\nidx_paths = sorted(ART_DIR.glob(\"test_trace_index_fold*.parquet\"))\nassert len(idx_paths) > 0\n\ndfs = []\nfor p in idx_paths:\n    d = pd.read_parquet(p)[[\"base_id\",\"lead\",\"crop_path\",\"trace_px_path\",\"mean_trace_conf\"]].copy()\n    d[\"base_id\"] = d[\"base_id\"].astype(str)\n    d[\"lead\"] = d[\"lead\"].astype(str)\n    d[\"crop_path\"] = d[\"crop_path\"].astype(str)\n    d[\"trace_px_path\"] = d[\"trace_px_path\"].astype(str)\n    dfs.append(d)\ndf_idx = pd.concat(dfs, ignore_index=True)\n\n# ---- map II_LONG -> II (submission expects lead II); prioritize II_LONG over short II\ndf_idx[\"lead_sub\"] = df_idx[\"lead\"].where(df_idx[\"lead\"].ne(\"II_LONG\"), \"II\")\ndf_idx[\"prio\"] = (df_idx[\"lead\"].eq(\"II_LONG\")).astype(np.int8)\ndf_idx = df_idx.sort_values([\"base_id\",\"lead_sub\",\"prio\",\"mean_trace_conf\"]).drop_duplicates(\n    [\"base_id\",\"lead_sub\",\"trace_px_path\"], keep=\"last\"\n)\n\n# ---- build per (base_id, lead_sub) ensemble list + crop_path\ng = df_idx.groupby([\"base_id\",\"lead_sub\"], sort=False)\ndf_pack = g.agg(\n    crop_path=(\"crop_path\", \"last\"),\n    trace_list=(\"trace_px_path\", list),\n).reset_index()\ndf_pack[\"key\"] = df_pack[\"base_id\"] + \"|\" + df_pack[\"lead_sub\"]\n\n# ---- ensure submission keys exist in pack\nkeys_needed = pd.Index(df_sub[\"key\"].unique())\nkeys_have = pd.Index(df_pack[\"key\"].unique())\nassert keys_needed.isin(keys_have).all()\n\n# ---- grid scale estimate (px/mm) per crop; fill failures with global median\ndef estimate_px_per_mm_y(crop_path: str) -> float:\n    img = cv2.imread(crop_path, cv2.IMREAD_GRAYSCALE)\n    assert img is not None\n    img = cv2.GaussianBlur(img, (3,3), 0)\n    ed = cv2.Canny(img, 50, 150)\n    proj = ed.sum(axis=1).astype(np.float32)\n    proj = proj - np.median(proj)\n    spec = np.abs(np.fft.rfft(proj))\n    k = np.arange(spec.shape[0], dtype=np.int32)\n    spec[0] = 0.0\n    N = float(proj.shape[0])\n    # period = N/k, keep plausible band [3px .. 60px]\n    period = N / np.maximum(k, 1)\n    mask = (k > 0) & (period >= 3.0) & (period <= 60.0)\n    spec_m = np.where(mask, spec, -1.0)\n    k_best = int(np.argmax(spec_m))\n    per = float(N / max(k_best, 1))\n    per = float(np.where(k_best > 0, per, np.nan))\n    # heuristic: if detected period looks like big-square (~5mm), convert to 1mm by /5\n    per_small = per / (1.0 + 4.0 * float(per > 15.0))\n    return float(per_small)\n\npxmm = []\nfor p in tqdm(df_pack[\"crop_path\"].values, total=len(df_pack), desc=\"Estimating grid scale (px/mm)\"):\n    try:\n        pxmm.append(estimate_px_per_mm_y(p))\n    except Exception:\n        pxmm.append(np.nan)\npxmm = np.array(pxmm, dtype=np.float32)\npxmm_med = float(np.nanmedian(pxmm))\npxmm_filled = np.where(np.isfinite(pxmm), pxmm, pxmm_med).astype(np.float32)\ndf_pack[\"px_per_mm_y\"] = pxmm_filled\n\n# ---- prepare fast lookup: key -> (trace_list, crop_path, px_per_mm_y)\npack = df_pack.set_index(\"key\")[[\"trace_list\",\"crop_path\",\"px_per_mm_y\"]]\n\n# ---- fill submission values groupwise (no per-row python loop)\nvalues = np.empty(len(df_sub), dtype=np.float32)\nvalues.fill(np.nan)\n\ndef resample_to_len(v: np.ndarray, n: int) -> np.ndarray:\n    x0 = np.linspace(0.0, 1.0, num=v.size, dtype=np.float32)\n    x1 = np.linspace(0.0, 1.0, num=n, dtype=np.float32)\n    return np.interp(x1, x0, v).astype(np.float32)\n\nfor key, idx in tqdm(df_sub.groupby(\"key\", sort=False).indices.items(), total=df_sub[\"key\"].nunique(), desc=\"Building values\"):\n    row_ids = df_sub[\"row_id\"].values[idx].astype(np.int32)\n    need_n = int(row_ids.max() + 1)\n\n    tr_list = pack.at[key, \"trace_list\"]\n    px_per_mm_y = float(pack.at[key, \"px_per_mm_y\"])\n\n    ys = []\n    for tp in tr_list:\n        ys.append(np.load(tp).astype(np.float32))\n    y = np.mean(np.stack(ys, axis=0), axis=0).astype(np.float32)\n\n    baseline = float(np.median(y))\n    mv_per_px = 1.0 / (10.0 * px_per_mm_y)  # 10 mm per 1 mV\n    v = ((baseline - y) * mv_per_px).astype(np.float32)\n\n    v_rs = resample_to_len(v, need_n)\n    values[idx] = v_rs[row_ids]\n\ndf_out = df_sub[[\"id\"]].copy()\ndf_out[\"value\"] = values.astype(np.float32)\n\n# ---- QA (hard checks)\nassert np.isfinite(df_out[\"value\"].values).all()\nassert df_out[\"id\"].is_unique\nassert len(df_out) == len(df_sub)\n\nqa = {\n    \"n_rows\": int(len(df_out)),\n    \"value_min\": float(np.min(df_out[\"value\"].values)),\n    \"value_max\": float(np.max(df_out[\"value\"].values)),\n    \"value_mean\": float(np.mean(df_out[\"value\"].values)),\n    \"value_std\": float(np.std(df_out[\"value\"].values)),\n    \"px_per_mm_y_median_used\": float(pxmm_med),\n    \"n_pxmm_filled\": int(np.sum(~np.isfinite(pxmm))),\n    \"n_folds_ensembled\": int(len(idx_paths)),\n}\n\n# ---- save submission\nsub_pq = SUB_DIR / \"submission.parquet\"\nsub_csv = SUB_DIR / \"submission.csv\"\ndf_out.to_parquet(sub_pq, index=False)\ndf_out.to_csv(sub_csv, index=False)\n\n(Path(ART_DIR) / \"stage7_qa.json\").write_text(json.dumps(qa, indent=2))\n\nprint(\"OK | Saved:\")\nprint(\" -\", sub_pq)\nprint(\" -\", sub_csv)\nprint(\" -\", ART_DIR / \"stage7_qa.json\")\nprint(\"QA:\", qa)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-21T22:22:14.176Z"}},"outputs":[],"execution_count":null}]}